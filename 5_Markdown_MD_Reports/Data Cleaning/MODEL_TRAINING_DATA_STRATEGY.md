# Model Training Data Strategy

## üìä **Recommended Approach**

### **Primary: Database-First Training** ‚úÖ (RECOMMENDED)

**Why Database?**
- ‚úÖ **Fastest**: Database queries are optimized with indexes
- ‚úÖ **Flexible**: Easy filtering by league, date range, team
- ‚úÖ **Real-time**: Always uses latest cleaned data
- ‚úÖ **Memory efficient**: Can stream data in batches
- ‚úÖ **Already implemented**: Your Dixon-Coles model loads from database

**How It Works:**
```python
# In model training code
from app.services.data_preparation import DataPreparationService

service = DataPreparationService(db)

# Load training data directly from database
df = service.load_training_data(
    league_codes=['E0', 'SP1', 'D1'],  # Premier League, La Liga, Bundesliga
    min_date=datetime(2020, 1, 1),
    max_date=datetime.now(),
    min_matches_per_team=10,
    source="database"  # ‚Üê Database is fastest
)
```

---

### **Secondary: File-Based Training** (Backup/Portability)

**When to Use Files:**
- ‚úÖ **Offline training**: When database unavailable
- ‚úÖ **Data sharing**: Export for external analysis
- ‚úÖ **Backup**: Archive cleaned data
- ‚úÖ **Jupyter notebooks**: Easy to load and explore

**File Format Recommendation:**

#### **1. CSV Files** (Human-Readable)
- ‚úÖ Easy to inspect and debug
- ‚úÖ Universal compatibility
- ‚úÖ Works in Excel, Python, R, etc.
- ‚ùå Larger file size (~10-50MB per league)
- ‚ùå Slower I/O for large datasets

#### **2. Parquet Files** (ML-Optimized) ‚≠ê **RECOMMENDED**
- ‚úÖ **50-80% smaller** than CSV
- ‚úÖ **5-10x faster** I/O
- ‚úÖ Columnar format (perfect for ML)
- ‚úÖ Built-in compression
- ‚úÖ Preserves data types
- ‚ùå Requires `pyarrow` library
- ‚ùå Not human-readable

**Recommendation: Export BOTH formats**
- CSV for human inspection
- Parquet for ML training

---

## üóÇÔ∏è **File Structure**

### **Current Structure:**
```
data/
‚îú‚îÄ‚îÄ 1_data_ingestion/          # Raw downloaded data (by batch)
‚îÇ   ‚îú‚îÄ‚îÄ batch_176_Premier_League/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ E0_1920.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ E0_2021.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ batch_177_Championship/
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ 2_Cleaned_data/            # Combined cleaned data (for training)
    ‚îú‚îÄ‚îÄ E0_Premier_League_all_seasons.csv
    ‚îú‚îÄ‚îÄ E0_Premier_League_all_seasons.parquet
    ‚îú‚îÄ‚îÄ SP1_La_Liga_all_seasons.csv
    ‚îú‚îÄ‚îÄ SP1_La_Liga_all_seasons.parquet
    ‚îî‚îÄ‚îÄ ...
```

### **Benefits:**
- ‚úÖ **One file per league** = Easy to load entire league history
- ‚úÖ **All seasons combined** = No need to merge multiple files
- ‚úÖ **Already cleaned** = Phase 1 cleaning applied
- ‚úÖ **Ready for training** = Optimized format

---

## üöÄ **Implementation**

### **1. Prepare Training Data Files**

**API Endpoint:**
```bash
POST /api/data/prepare-training-data
{
  "league_codes": ["E0", "SP1"],  # Optional: None = all leagues
  "format": "both"  # "csv", "parquet", or "both"
}
```

**Python Code:**
```python
from app.services.data_preparation import DataPreparationService

service = DataPreparationService(db)

# Prepare single league
stats = service.prepare_league_data("E0", format="both")
# Creates: E0_Premier_League_all_seasons.csv + .parquet

# Prepare all leagues
summary = service.prepare_all_leagues(format="both")
```

---

### **2. Load Data for Training**

#### **Option A: Database (Recommended)**
```python
from app.services.data_preparation import DataPreparationService

service = DataPreparationService(db)

# Load from database (fastest)
df = service.load_training_data(
    league_codes=['E0', 'SP1', 'D1'],
    min_date=datetime(2020, 1, 1),
    source="database"  # ‚Üê Fastest
)

# Train Dixon-Coles model
from app.models.dixon_coles import train_team_strengths
team_strengths = train_team_strengths(df, params)
```

#### **Option B: Files (Backup)**
```python
# Load from Parquet files (faster than CSV)
df = service.load_training_data(
    league_codes=['E0', 'SP1'],
    source="file"  # ‚Üê Loads from 2_Cleaned_data/
)
```

---

## üìà **Performance Comparison**

| Method | Speed | File Size | Use Case |
|--------|-------|-----------|----------|
| **Database** | ‚ö°‚ö°‚ö°‚ö°‚ö° Fastest | N/A | Production training |
| **Parquet** | ‚ö°‚ö°‚ö°‚ö° Fast | 50-80% smaller | Offline training |
| **CSV** | ‚ö°‚ö° Medium | Larger | Human inspection |

---

## üéØ **Training Workflow**

### **Recommended Workflow:**

```
1. Data Ingestion
   ‚Üì
   Download ‚Üí Clean ‚Üí Save CSV ‚Üí Insert to DB
   
2. Prepare Training Files (Optional - for backup/portability)
   ‚Üì
   POST /api/data/prepare-training-data
   Combines all seasons per league
   Exports CSV + Parquet to 2_Cleaned_data/
   
3. Model Training (Primary Method)
   ‚Üì
   Load from Database (fastest)
   Filter by league/date/team
   Train Dixon-Coles model
   
4. Alternative: Load from Files
   ‚Üì
   Load Parquet files (if DB unavailable)
   Train model
```

---

## üí° **Best Practices**

### **1. Database-First Approach**
- ‚úÖ Always load training data from database
- ‚úÖ Use file exports only for backup/portability
- ‚úÖ Database queries are optimized with indexes

### **2. File Format Choice**
- ‚úÖ **Parquet** for ML training (faster, smaller)
- ‚úÖ **CSV** for human inspection (readable)
- ‚úÖ Export both formats for flexibility

### **3. Data Preparation**
- ‚úÖ Run `prepare-training-data` after major data ingestion
- ‚úÖ Keep files in sync with database
- ‚úÖ Use Parquet for large datasets (>100K rows)

### **4. Training Data Loading**
- ‚úÖ Filter by date range (e.g., last 7 years)
- ‚úÖ Filter by league (train per league or combined)
- ‚úÖ Filter teams with minimum matches (e.g., >= 10 matches)

---

## üîß **Configuration**

### **In `app/config.py`:**
```python
# Data Preparation Configuration
COMBINE_SEASONS_PER_LEAGUE: bool = True  # Combine all seasons
USE_PARQUET_FORMAT: bool = True  # Export Parquet files
TRAINING_MIN_MATCHES_PER_TEAM: int = 10  # Filter threshold
```

---

## üìù **Example Usage**

### **1. Prepare Training Files**
```python
from app.services.data_preparation import prepare_training_data
from app.db.session import get_db

db = next(get_db())
stats = prepare_training_data(
    db=db,
    league_codes=['E0', 'SP1'],  # Premier League, La Liga
    output_format="both"  # CSV + Parquet
)

print(f"Prepared {stats['total_matches']} matches")
print(f"Files: {stats['files_created']}")
```

### **2. Load for Training**
```python
from app.services.data_preparation import DataPreparationService
from datetime import datetime

service = DataPreparationService(db)

# Load from database (recommended)
df = service.load_training_data(
    league_codes=['E0'],
    min_date=datetime(2020, 1, 1),
    min_matches_per_team=10,
    source="database"
)

print(f"Loaded {len(df)} matches for training")
```

---

## ‚úÖ **Summary**

**Answer to Your Questions:**

1. **Will files be combined per league?** ‚úÖ **YES**
   - One file per league: `{league_code}_{league_name}_all_seasons.csv/.parquet`
   - All seasons combined in single file

2. **CSV or Parquet?** ‚úÖ **BOTH**
   - CSV for human inspection
   - Parquet for ML training (recommended)

3. **How will model training be done?** ‚úÖ **DATABASE-FIRST**
   - Primary: Load from database (fastest)
   - Backup: Load from Parquet files (if needed)

4. **Whole data or bits?** ‚úÖ **FLEXIBLE**
   - Can filter by league, date range, team
   - Can load all leagues or specific ones
   - Can stream in batches for large datasets

---

**Status:** ‚úÖ **READY TO USE**

The data preparation service is implemented and ready. Use database-first approach for training, with file exports as backup/portability option.

