# Ingestion Log Columns Verification

## Database Schema

The `ingestion_logs` table has the following columns:

```sql
CREATE TABLE IF NOT EXISTS ingestion_logs (
    id              SERIAL PRIMARY KEY,              -- Auto-generated
    source_id       INTEGER REFERENCES data_sources(id),
    started_at      TIMESTAMPTZ DEFAULT now(),       -- Auto-generated
    completed_at    TIMESTAMPTZ,
    status          VARCHAR DEFAULT 'running',        -- 'running', 'completed', 'failed'
    records_processed INTEGER DEFAULT 0,
    records_inserted INTEGER DEFAULT 0,
    records_updated INTEGER DEFAULT 0,
    records_skipped INTEGER DEFAULT 0,
    error_message   TEXT,
    logs            JSONB,
    created_at      TIMESTAMPTZ NOT NULL DEFAULT now() -- Auto-generated
);
```

## Column Population Status

### ✅ Auto-Generated Columns (No Code Required)
- **`id`**: SERIAL PRIMARY KEY - Auto-generated by database
- **`started_at`**: TIMESTAMPTZ DEFAULT now() - Auto-generated by database
- **`created_at`**: TIMESTAMPTZ NOT NULL DEFAULT now() - Auto-generated by database

### ✅ Columns Set on Creation

**Location:** `app/api/data.py` (batch_download) and `app/services/data_ingestion.py` (ingest_csv, ingest_all_seasons)

- **`source_id`**: ✅ Set when creating `IngestionLog` instance
  ```python
  ingestion_log = IngestionLog(
      source_id=data_source.id,
      status="running"
  )
  ```

- **`status`**: ✅ Set to "running" on creation, updated to "completed" or "failed" later

### ✅ Columns Set on Completion

**Location:** `app/services/data_ingestion.py` (ingest_csv method, lines 278-305)

- **`completed_at`**: ✅ Set to `datetime.now()` when ingestion completes
  ```python
  ingestion_log.completed_at = datetime.now()
  ```

- **`records_processed`**: ✅ Accumulated from CSV processing
  ```python
  ingestion_log.records_processed = (ingestion_log.records_processed or 0) + stats["processed"]
  ```

- **`records_inserted`**: ✅ Accumulated from CSV processing
  ```python
  ingestion_log.records_inserted = (ingestion_log.records_inserted or 0) + stats["inserted"]
  ```

- **`records_updated`**: ✅ Accumulated from CSV processing
  ```python
  ingestion_log.records_updated = (ingestion_log.records_updated or 0) + stats["updated"]
  ```

- **`records_skipped`**: ✅ Accumulated from CSV processing
  ```python
  ingestion_log.records_skipped = (ingestion_log.records_skipped or 0) + stats["skipped"]
  ```

- **`error_message`**: ✅ Set to None if no errors, or concatenated error messages if errors occur
  ```python
  ingestion_log.error_message = "\n".join(existing_errors[:10]) if existing_errors else None
  ```

- **`logs`**: ✅ Set to JSON object with detailed information
  ```python
  ingestion_log.logs = {
      "errors": existing_errors[:50],
      "batch_number": batch_number,
      "files": batch_files  # List of all CSV files processed
  }
  ```

## Code Locations

### 1. Single Download (`/api/data/refresh`)

**File:** `app/api/data.py` (lines 56-127)

- Creates `IngestionLog` inside `ingest_from_football_data()` → `ingest_csv()` or `ingest_all_seasons()`
- All columns populated by `ingest_csv()` method

### 2. Batch Download (`/api/data/batch-download`)

**File:** `app/api/data.py` (lines 226-359)

- Creates `IngestionLog` per league (line 286-289)
- Passes `batch_number` to `ingest_from_football_data()` (line 301)
- `ingest_csv()` updates the existing log with detailed stats
- Endpoint refreshes log and ensures all fields are set (lines 305-332)

### 3. CSV Ingestion (`ingest_csv` method)

**File:** `app/services/data_ingestion.py` (lines 66-326)

- Creates `IngestionLog` if `batch_number` is None (lines 103-109)
- Or finds existing log if `batch_number` is provided (lines 112-126)
- Updates all metric columns (lines 278-305)
- Sets `error_message` and `logs` JSON (lines 300-305)

### 4. All Seasons Ingestion (`ingest_all_seasons` method)

**File:** `app/services/data_ingestion.py` (lines 379-465)

- Creates `IngestionLog` if `batch_number` is None (lines 407-413)
- Calls `ingest_csv()` multiple times with same `batch_number`
- Each call accumulates stats in the same log record

## Error Handling

### On Success
- `status` = "completed"
- `completed_at` = current timestamp
- All metric fields populated
- `error_message` = None (if no errors) or truncated error list
- `logs` = detailed JSON with file list and errors

### On Failure
**Location:** `app/services/data_ingestion.py` (lines 320-326) and `app/api/data.py` (lines 332-338)

- `status` = "failed"
- `completed_at` = current timestamp
- `error_message` = exception message
- Metric fields remain at defaults (0)

## Verification Checklist

- [x] `id` - Auto-generated ✅
- [x] `source_id` - Set on creation ✅
- [x] `started_at` - Auto-generated ✅
- [x] `completed_at` - Set on completion ✅
- [x] `status` - Set to "running", then "completed" or "failed" ✅
- [x] `records_processed` - Accumulated during CSV processing ✅
- [x] `records_inserted` - Accumulated during CSV processing ✅
- [x] `records_updated` - Accumulated during CSV processing ✅
- [x] `records_skipped` - Accumulated during CSV processing ✅
- [x] `error_message` - Set to None or error text ✅
- [x] `logs` - Set to JSON with detailed information ✅
- [x] `created_at` - Auto-generated ✅

## Summary

**All columns in the `ingestion_logs` table are being properly populated.**

- Auto-generated columns (`id`, `started_at`, `created_at`) are handled by the database
- Required columns (`source_id`, `status`) are set on creation
- Metric columns (`records_*`) are accumulated during CSV processing
- Metadata columns (`completed_at`, `error_message`, `logs`) are set on completion
- Error handling properly sets `status` and `error_message` on failure

## Recent Fix

**Issue:** In `batch_download` endpoint, the detailed `logs` JSON from `ingest_csv` was being overwritten with a simpler structure.

**Fix:** Updated `batch_download` to refresh the log from DB and preserve/merge existing logs instead of overwriting them. This ensures all detailed information (errors, file list) from `ingest_csv` is preserved.

